import pymupdf  # PyMuPDF
import json
from openai import OpenAI
import time
from concurrent.futures import ThreadPoolExecutor, as_completed


client = OpenAI()

# ğŸ“„ PDF ë¬¸ì„œì˜ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def extractTextByPage(pdf_path):
    """
    pymupdfë¡œ PDFë¥¼ ì—´ê³  ê° í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ê° í•­ëª©ì€ {'page': í˜ì´ì§€ ë²ˆí˜¸, 'text': í…ìŠ¤íŠ¸} í˜•ì‹ì…ë‹ˆë‹¤.
    """
    pages_text = []

    # PDF ë¬¸ì„œë¥¼ ì—´ê³  with ë¬¸ìœ¼ë¡œ ìë™ ë‹«ê¸° ì²˜ë¦¬
    with pymupdf.open(pdf_path) as doc:
        for i in range(len(doc)):
            page = doc[i]
            text = page.get_text()  # ê¸°ë³¸ê°’ "text"ëŠ” ì¤„ë°”ê¿ˆ í¬í•¨ëœ ì¼ë°˜ í…ìŠ¤íŠ¸
            pages_text.append({
                "page": i + 1,  # ì‚¬ëŒ ê¸°ì¤€ì˜ 1-based page ë²ˆí˜¸
                "text": text.strip()
            })

    return pages_text


def generateSystemPrompt(summary_language="English"):
    """
    ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    summary_language: ìš”ì•½ ê²°ê³¼ ì–¸ì–´ ("English", "Korean" ë“±)
    """
    return f'''You are a professional assistant designed to generate high-quality contextual summaries for each page of a PDF document. Your goal is to help improve machine translation quality by providing concise, informative summaries that preserve the core meaning of each page.

For each page:
- Read the content carefully and understand the main idea.
- Write a summary in {summary_language} using no more than 3 sentences.
- Focus on conveying the key message, avoiding unnecessary detail or repetition.
- Be clear, concise, and faithful to the original content.

The output should be in JSON format, with the following structure:

{{
  "summaries": [
    {{
      "page": 1,
      "summary": "This page explains..."
    }},
    ...
  ]
}}

- The top-level JSON object must contain a key named "summaries".
- "summaries" must be a list of summary objects.
- Each summary object must include "page" (integer) and "summary" (string).
- Do not include any keys other than "summaries" at the top level.

Do not translate. Do not analyze formatting. Do not paraphrase unnecessarily. Just summarize the core message per page.'''


# ğŸ¤– GPT ëª¨ë¸ì—ê²Œ í•˜ë‚˜ì˜ ì²­í¬(ì˜ˆ: 10í˜ì´ì§€)ë¥¼ ìš”ì•½ ìš”ì²­í•˜ëŠ” í•¨ìˆ˜
def summarizePageChunk(pages, summary_language="English"):
    """
    í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ GPTì—ê²Œ ìš”ì²­í•˜ê³ , ì§€ì •í•œ ì–¸ì–´ë¡œ ìš”ì•½ì„ ë°›ìŠµë‹ˆë‹¤.
    ì‘ë‹µì€ {"summaries": [...]} í˜•íƒœì˜ ë¬¸ìì—´ì´ë¯€ë¡œ, JSON íŒŒì‹± í›„ summaries ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ìš”ì•½ ì–¸ì–´ ì§€ì • í¬í•¨)
    system_prompt = generateSystemPrompt(summary_language)

    # í˜ì´ì§€ë³„ ì…ë ¥ í…ìŠ¤íŠ¸ êµ¬ì„±
    input_text = "\n\n".join([
        f"Page {p['page']}:\n{p['text']}" for p in pages
    ])

    # GPT ëª¨ë¸ í˜¸ì¶œ
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": input_text}
        ],
        response_format={
            "type": "json_schema",
            "json_schema": {
                "name": "summary_response",
                "strict": True,
                "schema": {
                    "type": "object",
                    "properties": {
                        "summaries": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "page": { "type": "integer" },
                                    "summary": { "type": "string" }
                                },
                                "required": ["page", "summary"],
                                "additionalProperties": False
                            }
                        }
                    },
                    "required": ["summaries"],
                    "additionalProperties": False
                }
            }
        }
    )

    # ì‘ë‹µì˜ contentëŠ” ë¬¸ìì—´ í˜•íƒœì˜ JSON â†’ Python ê°ì²´ë¡œ íŒŒì‹±
    parsed = json.loads(response.choices[0].message.content)

    # "summaries" í‚¤ë¥¼ ë°˜í™˜
    return parsed["summaries"]




def summarizePdfInChunks(pdf_path, summary_language="English", chunk_size=15):
    """
    ì „ì²´ PDF íŒŒì¼ì„ ì—¬ëŸ¬ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ GPT ëª¨ë¸ì—ê²Œ ìš”ì•½ì„ ìš”ì²­í•©ë‹ˆë‹¤.
    - chunk_size ë§¤ê°œë³€ìˆ˜ë¥¼ í†µí•´ í•œ ë²ˆì— ëª‡ í˜ì´ì§€ì”© ìš”ì•½í• ì§€ ì¡°ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ê° ë¸”ë¡ì— ëŒ€í•´ ìš”ì•½ì´ ì¼ë¶€ ëˆ„ë½ëœ ê²½ìš°, í•´ë‹¹ ìœ„ì¹˜ì— ë¹ˆ summaryë¥¼ ë„£ìŠµë‹ˆë‹¤.
    
    Returns:
        List[Dict]: {"page": int, "summary": str} ë¦¬ìŠ¤íŠ¸ (page ì˜¤ë¦„ì°¨ìˆœ)
    """
    all_pages = extractTextByPage(pdf_path)
    all_summaries = []

    for i in range(0, len(all_pages), chunk_size):
        chunk = all_pages[i:i+chunk_size]
        page_numbers = [p["page"] for p in chunk]
        page_range = f"{page_numbers[0]}â€“{page_numbers[-1]}"
        print(f"ğŸ“˜ Summarizing pages {page_range}...")

        try:
            # GPT ì‘ë‹µ ìš”ì²­
            summaries = summarizePageChunk(chunk, summary_language)
            summary_dict = {item["page"]: item["summary"] for item in summaries}

            # ğŸ‘‰ ìˆœì„œë¥¼ ë³´ì¥í•˜ë©° ëˆ„ë½ëœ í˜ì´ì§€ëŠ” ""ë¡œ ì±„ì›€
            ordered_chunk_summary = []
            for page in page_numbers:
                ordered_chunk_summary.append({
                    "page": page,
                    "summary": summary_dict.get(page, "")
                })

            all_summaries.extend(ordered_chunk_summary)
            print(f"âœ… Completed chunk {page_range} with {len(ordered_chunk_summary)} summaries")

        except Exception as e:
            print(f"âŒ Failed to summarize pages {page_range}: {e}")
            # ì „ì²´ ì²­í¬ ì‹¤íŒ¨ ì‹œ ì „ë¶€ ë¹ˆ summary
            for page in page_numbers:
                all_summaries.append({
                    "page": page,
                    "summary": ""
                })

    return all_summaries



def summarizePdfInChunksParallel(pdf_path, summary_language="English", chunk_size=15, max_workers=30):
    """
    PDFë¥¼ chunk ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³  ê° ì²­í¬ë¥¼ ë³‘ë ¬ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
    ì‹¤íŒ¨í•œ ì²­í¬ëŠ” ê° í˜ì´ì§€ì— ë¹ˆ summaryë¥¼ ì‚½ì…í•©ë‹ˆë‹¤.

    Parameters:
        pdf_path (str): PDF íŒŒì¼ ê²½ë¡œ
        summary_language (str): ìš”ì•½ ì–¸ì–´
        chunk_size (int): chunk ë‹¹ í˜ì´ì§€ ìˆ˜
        max_workers (int): ë³‘ë ¬ ì‘ì—… ìµœëŒ€ ì“°ë ˆë“œ ìˆ˜

    Returns:
        List[Dict]: {"page": int, "summary": str} ë¦¬ìŠ¤íŠ¸
    """
    all_pages = extractTextByPage(pdf_path)

    # chunk ë‹¨ìœ„ë¡œ ë¶„í• 
    chunks = [
        all_pages[i:i+chunk_size]
        for i in range(0, len(all_pages), chunk_size)
    ]

    results = []

    def processChunk(chunk):
        page_numbers = [p["page"] for p in chunk]
        page_range = f"{page_numbers[0]}â€“{page_numbers[-1]}"
        print(f"ğŸ“˜ [Thread] Summarizing pages {page_range}...")

        try:
            summaries = summarizePageChunk(chunk, summary_language)
            summary_dict = {item["page"]: item["summary"] for item in summaries}
            ordered_chunk_summary = [
                {"page": page, "summary": summary_dict.get(page, "")}
                for page in page_numbers
            ]
            print(f"âœ… [Thread] Completed pages {page_range}")
            return ordered_chunk_summary

        except Exception as e:
            print(f"âŒ [Thread] Failed pages {page_range}: {e}")
            return [{"page": page, "summary": ""} for page in page_numbers]

    # ë³‘ë ¬ ì‹¤í–‰
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(processChunk, chunk) for chunk in chunks]

        for future in as_completed(futures):
            results.extend(future.result())

    # ì •ë ¬ ë³´ì¥
    return sorted(results, key=lambda x: x["page"])





def summarizeTest(pdf_path, summary_language="English"):
    pages = extractTextByPage(pdf_path)
    summaries = summarizePageChunk(pages, summary_language)
    
    for item in summaries:
        page = item["page"]
        summary = item["summary"]
        print(f"\nâœ… Page {page}")
        print(f"   {summary}")
        