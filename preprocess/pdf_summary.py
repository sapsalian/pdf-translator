import pymupdf  # PyMuPDFë¥¼ ì‚¬ìš©í•˜ì—¬ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
import json
from openai import OpenAI  # OpenAI GPT í˜¸ì¶œì„ ìœ„í•œ í´ë¼ì´ì–¸íŠ¸
import time
from collections import defaultdict, Counter  # ìš©ì–´ ë¹ˆë„ ê³„ì‚°ìš©
from concurrent.futures import ThreadPoolExecutor, as_completed  # ë³‘ë ¬ ì²˜ë¦¬

client = OpenAI()

# ğŸ“„ PDF ë¬¸ì„œì˜ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def extractTextByPage(pdf_path):
    pages_text = []
    with pymupdf.open(pdf_path) as doc:
        for i in range(len(doc)):
            text = doc[i].get_text().strip()
            pages_text.append({"page": i + 1, "text": text})
    return pages_text

# ğŸ§  ë²ˆì—­ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ ìš”ì•½ + ìš©ì–´ì§‘ ìƒì„± ìš”ì²­ í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
def generateCombinedSystemPrompt(source_language="English", target_language="Korean"):
    return f'''You are an expert assistant whose job is to support high-quality and consistent translation from {source_language} to {target_language}.

You will be given a range of pages from a document. Your task has two parts:

1. ğŸ“„ **Summarize** the key content of each page in **no more than 3 sentences per page**.
   - Focus on the main message of the page.
   - Do not include minor details, formatting info, or examples unless they are essential.
   - Write clear, concise, and faithful summaries in {source_language}.
   - The summaries will be used as context for translation.

2. ğŸ“˜ **Extract a glossary** of terms that require consistent translation.
   - Return a JSON object (`terms`) where:
     - Each **key** is an English term or phrase (can be multiple words) that appears multiple times or has different possible meanings in context.
     - Each **value** is its consistent translation in {target_language}.
   - Include terms that:
     - Might be translated inconsistently depending on context.
     - Are domain-specific technical terms, abstract concepts, or ambiguous expressions.
     - Appear very frequently in the document, even if their meaning is clear, to help enforce translation consistency.

   - For **named entities** (e.g., GPT-4, API, LLM, ChatGPT, OpenAI):
     - **Do not translate them**. Keep them exactly as-is. Their value should equal the key.

3. ğŸ§¾ **Output Format**
Return only a JSON object like this (do not include any explanation, commentary, or extra text):

{{
  "terms": {{
    "embedding": "ì„ë² ë”©",
    "language model": "ì–¸ì–´ ëª¨ë¸",
    "GPT-4": "GPT-4"
  }},
  "summaries": [
    {{ "page": 1, "summary": "..." }},
    {{ "page": 2, "summary": "..." }}
  ]
}}

Do not change the structure.
Do not return anything except a valid JSON object with exactly two top-level keys: `terms` and `summaries`.
'''



# ğŸ¤– GPT í˜¸ì¶œ: ê° ì²­í¬ì— ëŒ€í•´ ìš”ì•½ê³¼ ìš©ì–´ì§‘ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def summarizeChunkWithTerms(pages, source_language="English", target_language="Korean"):
    system_prompt = generateCombinedSystemPrompt(source_language, target_language)
    input_text = "\n\n".join([f"Page {p['page']}:\n{p['text']}" for p in pages])

    # GPT í˜¸ì¶œ ì‹œ, JSON ìŠ¤í‚¤ë§ˆ ëª…ì‹œí•˜ì—¬ ìš”ì•½ + ìš©ì–´ì§‘ êµ¬ì¡° ê°•ì œ
    response = client.chat.completions.create(
        model="gpt-4.1-mini",
        # temperature=0,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": input_text}
        ],
        response_format={
            "type": "json_schema",
            "json_schema": {
                "name": "summary_with_terms",
                "strict": True,
                "schema": {
                    "type": "object",
                    "properties": {
                        "summaries": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "page": {"type": "integer"},
                                    "summary": {"type": "string"}
                                },
                                "required": ["page", "summary"],
                                "additionalProperties": False
                            }
                        },
                        "terms": {
                            "type": "object",
                            "description": "A glossary mapping English terms to their Korean translations.",
                            "properties": {},
                            "additionalProperties": {"type": "string"}
                        }

                    },
                    "required": ["summaries", "terms"],
                    "additionalProperties": False
                }
            }
        }
    )

    parsed = json.loads(response.choices[0].message.content)
    return parsed["summaries"], parsed["terms"]

# ğŸ§  ì²­í¬ë³„ë¡œ ìˆ˜ì§‘ëœ ìš©ì–´ì§‘ì„ ë³‘í•©í•˜ëŠ” í•¨ìˆ˜ (ê°€ì¥ ìì£¼ ë“±ì¥í•œ ë²ˆì—­ ì„ íƒ)
def mergeGlossaries(glossaries):
    term_freq = defaultdict(Counter)
    for glossary in glossaries:
        for key, val in glossary.items():
            term_freq[key][val] += 1
    # ê°€ì¥ ë§ì´ ë“±ì¥í•œ ë²ˆì—­ì„ ì„ íƒ
    final_terms = {
        key: counter.most_common(1)[0][0] for key, counter in term_freq.items()
    }
    return final_terms

# ğŸ“˜ ì „ì²´ PDFë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ìš”ì•½í•˜ê³  ìš©ì–´ì§‘ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def summarizePdfInChunks(pdf_path, chunk_size=7, source_language="English", target_language="Korean"):
    # PDFì—ì„œ ëª¨ë“  í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
    all_pages = extractTextByPage(pdf_path)
    all_summaries = []  # ì „ì²´ ìš”ì•½ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
    all_glossaries = []  # ì „ì²´ ìš©ì–´ì§‘ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸

    # chunk_sizeë§Œí¼ í˜ì´ì§€ë¥¼ ì˜ë¼ì„œ ë°˜ë³µ ì²˜ë¦¬
    for i in range(0, len(all_pages), chunk_size):
        chunk = all_pages[i:i + chunk_size]  # í˜„ì¬ ì²­í¬
        page_numbers = [p["page"] for p in chunk]  # ì²­í¬ ë‚´ í˜ì´ì§€ ë²ˆí˜¸ ëª©ë¡
        page_range = f"{page_numbers[0]}â€“{page_numbers[-1]}"
        print(f"ğŸ“˜ Summarizing pages {page_range}...")

        try:
            # GPTë¡œ ìš”ì•½ê³¼ ìš©ì–´ì§‘ ìƒì„± ìš”ì²­
            summaries, glossary = summarizeChunkWithTerms(chunk, source_language, target_language)
            summary_dict = {item["page"]: item["summary"] for item in summaries}  # í˜ì´ì§€ ë²ˆí˜¸ ê¸°ë°˜ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±

            # ìˆœì„œ ìœ ì§€í•˜ë©° ëˆ„ë½ëœ í˜ì´ì§€ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
            ordered = [{"page": page, "summary": summary_dict.get(page, "")} for page in page_numbers]
            all_summaries.extend(ordered)
            all_glossaries.append(glossary)
        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ ì‹œ ëª¨ë“  í˜ì´ì§€ì— ë¹ˆ ìš”ì•½ í• ë‹¹
            print(f"âŒ Failed to summarize pages {page_range}: {e}")
            all_summaries.extend([{"page": page, "summary": ""} for page in page_numbers])

    # ì²­í¬ë³„ ìš©ì–´ì§‘ ë³‘í•© (ì¤‘ë³µ í‚¤ëŠ” ê°€ì¥ ë¹ˆë„ ë†’ì€ ë²ˆì—­ ì„ íƒ)
    merged_glossary = mergeGlossaries(all_glossaries)

    # ì •ë ¬ëœ ìš”ì•½ê³¼ ìµœì¢… ìš©ì–´ì§‘ ë°˜í™˜
    return {"term_dict": merged_glossary, "summaries": sorted(all_summaries, key=lambda x: x["page"])}

# âš¡ ì „ì²´ PDFë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ë©° ìš”ì•½ê³¼ ìš©ì–´ì§‘ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def summarizePdfInChunksParallel(pdf_path, chunk_size=7, max_workers=30, source_language="English", target_language="Korean"):
    all_pages = extractTextByPage(pdf_path)  # ëª¨ë“  í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    chunks = [all_pages[i:i + chunk_size] for i in range(0, len(all_pages), chunk_size)]  # ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• 
    results = []  # ë³‘ë ¬ ê²°ê³¼ ìˆ˜ì§‘ìš© ë¦¬ìŠ¤íŠ¸
    all_glossaries = []  # ë³‘ë ¬ ìƒì„±ëœ ìš©ì–´ì§‘ ë¦¬ìŠ¤íŠ¸

    # ê°œë³„ ì²­í¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜
    def processChunk(chunk):
        page_numbers = [p["page"] for p in chunk]
        page_range = f"{page_numbers[0]}â€“{page_numbers[-1]}"
        print(f"ğŸ“˜ [Thread] Summarizing pages {page_range}...")

        try:
            # GPT í˜¸ì¶œ: ìš”ì•½ ë° ìš©ì–´ì§‘ ìƒì„±
            summaries, glossary = summarizeChunkWithTerms(chunk, source_language, target_language)
            summary_dict = {item["page"]: item["summary"] for item in summaries}
            ordered = [{"page": page, "summary": summary_dict.get(page, "")} for page in page_numbers]
            all_glossaries.append(glossary)
            return ordered
        except Exception as e:
            # ì‹¤íŒ¨ ì‹œ ë¹ˆ ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´
            print(f"âŒ [Thread] Failed pages {page_range}: {e}")
            return [{"page": page, "summary": ""} for page in page_numbers]

    # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜í–‰
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(processChunk, chunk) for chunk in chunks]  # ì‘ì—… ì œì¶œ
        for future in as_completed(futures):  # ì™„ë£Œëœ ì‘ì—…ë¶€í„° ê²°ê³¼ ìˆ˜ì§‘
            results.extend(future.result())

    # ì „ì²´ ìš©ì–´ì§‘ ë³‘í•©
    merged_glossary = mergeGlossaries(all_glossaries)

    # ì •ë ¬ëœ ìš”ì•½ê³¼ ìš©ì–´ì§‘ ë°˜í™˜
    return {"term_dict": merged_glossary, "summaries": sorted(results, key=lambda x: x["page"])}





def summarizeTest(pdf_path, source_language="English", target_language="Korean"):
    # ì „ì²´ PDFë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ ìš”ì•½ ë° ìš©ì–´ì§‘ ìƒì„±
    result = summarizePdfInChunksParallel(
        pdf_path,
        chunk_size=7,
        max_workers=30,
        source_language=source_language,
        target_language=target_language
    )

    summaries = result["summaries"]
    terms = result["term_dict"]

    # ìš”ì•½ ê²°ê³¼ ì¶œë ¥
    for item in summaries:
        page = item["page"]
        summary = item["summary"]
        print(f"\nâœ… Page {page}")
        print(f"   {summary}")

    # ìš©ì–´ì§‘ ì¶œë ¥
    print("\nğŸ“˜ Glossary Terms")
    for term, translation in terms.items():
        print(f" - {term} â†’ {translation}")


        